
# Multimodal Search ML

---

# Milestone 3

## Overview

This milestone focuses on building the data pipeline for the MultimodalSearchML project. We automated the ingestion, validation, preprocessing, and preparation of the dataset using TFX, ensured proper data versioning with DVC, and managed features with Feast as our feature store.

---

## Data Ingestion

- **Library used**: TFX ExampleGen

- **Dataset used:** query_features_with_timestamp.csv

- **Task**: Ingest raw data into the pipeline for future steps (statistics, schema generation, validation, and transformation).

I created a TFX pipeline component for ingestion using CsvExampleGen. Then, I loaded the dataset from:

```bash
data/query/query_features_with_timestamp.csv
```

The dataset is already prepared with query_id, f0 to f767 features, and an event_timestamp column.

```python
from tfx.components import CsvExampleGen
from tfx.proto import example_gen_pb2

input_config = example_gen_pb2.Input(splits=[
    example_gen_pb2.Input.Split(name='train', pattern='query_features_with_timestamp.csv'),
])

example_gen = CsvExampleGen(input_base=data_path, input_config=input_config)
```

- The CsvExampleGen is used to automatically:

	- Ingest the CSV file

	- Convert it into TFRecords

	- Automatically split into train and eval sets

	- Store generated artifacts under artifacts/CsvExampleGen

Results:

- You can observe the ingested data under:

```bash
tfx_pipeline/artifacts/CsvExampleGen/examples/
```

and inside:

```mathematica
Split-train/
Split-eval/
```

Each containing generated data_tfrecord-* files.

---

## Data Validation 

To automatically generate statistics, infer a data schema, and detect anomalies in the ingested query dataset before applying transformations or training models.

I added three essential components:

1. StatisticsGen: Computes descriptive statistics on the ingested data.

2. SchemaGen: Automatically infers the schema from the statistics.

3. ExampleValidator: Detects data anomalies and schema drift.

```python
from tfx.components import StatisticsGen, SchemaGen, ExampleValidator

# Compute statistics
statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

# Infer schema
schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])

# Validate dataset against the inferred schema
example_validator = ExampleValidator(
    statistics=statistics_gen.outputs['statistics'],
    schema=schema_gen.outputs['schema']
)
```

Pipeline Flow:

- Statistics are automatically computed from the TFRecords generated by ExampleGen.

- SchemaGen infers a schema without any manual intervention.

- ExampleValidator checks if:

    - The data conforms to the inferred schema.

    - There are missing values, unusual distributions, or unexpected types.

Results:

- Statistics are stored under:

```bash
tfx_pipeline/artifacts/StatisticsGen/statistics/
```

- Schema is stored under:

```bash
tfx_pipeline/artifacts/SchemaGen/schema/
```

- This is the output of our schema: (schame.pbtxt)

```mathematica
feature {
  name: "event_timestamp"
  type: BYTES
  domain: "event_timestamp"
  presence {
    min_fraction: 1.0
    min_count: 1
  }
  shape {
    dim {
      size: 1
    }
  }
}
feature {
  name: "f0"
  type: FLOAT
  presence {
    min_fraction: 1.0
    min_count: 1
  }
  shape {
    dim {
      size: 1
    }
  }
}
feature {
  name: "f1"
  type: FLOAT
  presence {
    min_fraction: 1.0
    min_count: 1
  }
  shape {
    dim {
      size: 1
    }
  }
}
.....
.....
feature {
  name: "f767"
  type: FLOAT
  presence {
    min_fraction: 1.0
    min_count: 1
  }
  shape {
    dim {
      size: 1
    }
  }
}
feature {
  name: "query_id"
  type: INT
  presence {
    min_fraction: 1.0
    min_count: 1
  }
  shape {
    dim {
      size: 1
    }
  }
}
string_domain {
  name: "event_timestamp"
  value: "2025-03-27 07:01:04.332726+00:00"
}
```

- Anomalies report is stored under:

```bash
tfx_pipeline/artifacts/ExampleValidator/anomalies/
```

NB:

- This process is fully automated thanks to TensorFlow Data Validation (TFDV).
- The schema file (schema.pbtxt) will serve as input for the next Transform step.
- Any anomalies detected can be analyzed in TensorBoard or directly via the generated artifacts.


---

## Data Preprocessing and Feature Engineering

Prepare the dataset for model training by applying transformations, selecting useful features, and managing missing data using TensorFlow Transform (TFX Transform).

I built a dedicated preprocessing module responsible for:

- Selecting only useful columns (768 embedding features).

- Handling potential missing data.

- Preparing the data for modeling in a scalable and production-friendly way. 

Preprocessing Module (preprocessing.py):

```python
import tensorflow as tf
import tensorflow_transform as tft

def preprocessing_fn(inputs):
    # Keep only feature columns f0 to f767
    outputs = {}
    for i in range(768):
        col = f"f{i}"
        outputs[col] = inputs[col]

    # Keep the event timestamp as is
    outputs['event_timestamp'] = inputs['event_timestamp']

    return outputs
```

TFX Transform Component (in the pipeline):

```python
from tfx.components import Transform

transform = Transform(
    examples=example_gen.outputs['examples'],
    schema=schema_gen.outputs['schema'],
    module_file=module_file,  # preprocessing.py script
)
```

What this step achieves:

- Reduces the dataset to only the needed features (f0 - f767 + timestamp).
- Ensures data is formatted consistently.
- Scales and prepares data in a reproducible way for future serving.
- The transformed dataset is automatically split into:
	- Split-train/
	- Split-eval/

Output Directories:

- Transform Graph:

```bash
tfx_pipeline/artifacts/Transform/transform_graph/
```

- Transformed Dataset:

```bash
tfx_pipeline/artifacts/Transform/transformed_examples/
```

---

## Feature Store

In this step, I integrated Feast as a feature store to manage and serve features for the MultimodalSearchML project. The feature store plays a crucial role in centralizing, versioning, and serving feature data for both training and serving machine learning models.

Objectives:

- Organize, version, and serve query and product dataset features.
- Enable consistent feature access during both training and inference.
- Prepare for later integration with the future TFX pipeline model training phase.

Data Used:

- query_features_flat.csv
- product_features_flat.parquet

These datasets contain extracted features representing query embeddings and product embeddings computed using CLIP models.

I installed Feast in a dedicated environment:

```bash
pip install feast==0.40.1
```

Then, initialized the feature repository:

```bash
feast init feature_store/multimodal_features
```

I defined two main FeatureViews:

1. Query Feature View from query_features_flat.csv
2. Product Feature View from product_features_flat.parquet

Example from query_features_view.py:

```python
query_feature_view = FeatureView(
    name="query_features_view",
    entities=[query_entity],
    ttl=Duration(seconds=86400),
    schema=[Field(name=f"f{i}", dtype=Float32) for i in range(768)],
    source=query_source
)
```

Example from product_features_view.py:

```python
product_feature_view = FeatureView(
    name="product_features_view",
    entities=[product_entity],
    ttl=Duration(seconds=86400),
    schema=[Field(name=f"f{i}", dtype=Float32) for i in range(768)],
    source=product_source
)
```
 
#### Feature Store Registration

Once the FeatureViews were defined, we applied them:

```bash
feast apply
```

Feast registered:

- 2 Entities (query_id and product_id)
- 2 FeatureViews
- Features with embedding dimensions: (f0 to f767 for queries) and (f0 to f1535 for products)

Feature Materialization: 

I materialized features to be ready for offline and online serving:

```bash
feast materialize-incremental $(date -u +%Y-%m-%d)
```

Feast then generated:

- Offline feature data (for training pipelines)
- Online feature data (for future model serving)

Summary: 

- Both datasets were successfully managed in Feast.
- Redis was configured as an online store (development purpose).
- The setup ensures that during model training or serving, the exact same feature values are retrieved as during feature generation.

---

## Data Versioning (DVC)

In this step, I integrated DVC (Data Version Control) to manage and version the raw data used throughout the milestone. DVC provides a Git-like workflow for datasets, making sure that every experiment or pipeline execution always uses the correct version of the data.

Objectives

    Track the query and product datasets systematically.

    Avoid redundant storage.

    Enable reproducibility of experiments.

    Prepare the project for collaborative work and future experiments.

I versioned the raw data folder containing:

    query_features_flat.csv

    query_features_with_timestamp.csv

    product_features_flat.parquet

    product_image_urls.csv

    supp_product_image_urls.csv

I initialized DVC inside the project directory:

```bash
dvc init
```

This created the .dvc/ folder and the configuration files necessary to start versioning data.

Then, for tracking the dataset, I use the following command to track the whole folder:

```bash
dvc add Milestone\ 3/data/raw/
```

DVC created:

- raw.dvc file to track changes.
- .dvc/cache/ for storing dataset versions efficiently.

I also configured a local remote to store data artifacts:

```bash
dvc remote add -d localremote ~/dvcstore
dvc remote modify localremote type local
```

This ensures all future dataset versions will be pushed to ~/dvcstore safely.

The datasets were pushed into the local remote:

```bach
dvc push
```

This command:

- Stored the dataset version into the ~/dvcstore directory.

- Allowed us to share datasets without duplicating the data.

---

## Ingestion and Preprocessing Pipeline (TFX)

In this step, I designed and implemented a fully functional TFX (TensorFlow Extended) pipeline for ingesting, validating, and preparing the query dataset for modeling. We focused on separating ingestion, validation, and preprocessing clearly while ensuring the pipeline is clean, professional, and reproducible.

### Pipeline Overview

This pipeline included the following TFX components:

1. CsvExampleGen

2. StatisticsGen

3. SchemaGen

4. ExampleValidator

5. Transform

Each component generated useful artifacts under the artifacts/ directory automatically, maintaining the TFX structure of numbered runs and split directories (train/eval).

### CsvExampleGen

This component ingests the query_features_with_timestamp.csv dataset.

```python
input_config = example_gen_pb2.Input(splits=[
    example_gen_pb2.Input.Split(name="train", pattern="query_features_with_timestamp.csv"),
])
example_gen = CsvExampleGen(input_base=data_path, input_config=input_config)
```

It automatically:

- Split the dataset.

- Converted it into TFRecords under:

```swift
tfx_pipeline/artifacts/CsvExampleGen/examples/1/Split-train/
tfx_pipeline/artifacts/CsvExampleGen/examples/1/Split-eval/
```

### StatisticsGen

It computed descriptive statistics of the dataset:

```python
statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])
```

Generated:

- FeatureStats.pb containing complete statistics of all columns.
- Split under:

```swift
artifacts/StatisticsGen/statistics/{run_id}/Split-train/FeatureStats.pb
```

### SchemaGen

Automatically inferred the data schema:

```python
schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])
```

Generated:

- schema.pbtxt defining data types, expected values, and feature constraints.

- Saved under:

```bash
artifacts/SchemaGen/schema/{run_id}/schema.pbtxt
```

### ExampleValidator

Validated dataset against the inferred schema:

```python
example_validator = ExampleValidator(
    statistics=statistics_gen.outputs['statistics'],
    schema=schema_gen.outputs['schema']
)
```

Generated:

- SchemaDiff.pb showing anomalies and schema drifts.

- Stored under:

```swift
artifacts/ExampleValidator/anomalies/{run_id}/Split-train/SchemaDiff.pb
```

### Transform (Preprocessing)

I used a custom preprocessing module to:

- Remove unwanted features.

- Scale and normalize input features.

- Keep only the f0 to f767 (768 clip-text embeddings).

Example code:

```python
def preprocessing_fn(inputs):
    return {
        f"f{i}": inputs[f"f{i}"] for i in range(768)
    }
```

TFX Transform component:

```python
transform = Transform(
    examples=example_gen.outputs["examples"],
    schema=schema_gen.outputs["schema"],
    module_file=module_file,
)
```

Generated:

- transformed_examples/ (TFRecords ready for modeling)

- transform_graph/ containing transformation logic for serving


### Full Pipeline Execution

We executed the pipeline with:

```bash
python tfx_pipeline/scripts/run_ingest_query_pipeline.py
```

It completed successfully and populated the following:

- All component artifacts

- Automatically versioned runs (run_id folders)

- Split-aware artifacts (train/eval)

Example Final Pipeline Structure (auto-generated by TFX):

```css
artifacts/
    CsvExampleGen/
    StatisticsGen/
    SchemaGen/
    ExampleValidator/
    Transform/
```

---

## Data Visualization [Notebook](https://github.com/yassineMtg/MultimodalSearchML/blob/main/Milestone%203/notebooks/pipeline_summary.ipynb)

After running the ingestion and preprocessing pipeline, we prepared a dedicated Jupyter Notebook to:

1. Explore TFX-generated artifacts.

2. Visualize statistics, schema, and anomalies.

3. Confirm data correctness before training.

Notebook Objective:

Load pipeline artifacts automatically.

- Show summary statistics.

- Visualize feature distributions.

- Display schema.

- Report anomalies detected by ExampleValidator.

- Prepare the pipeline for potential monitoring (TensorBoard or other tools).

As It showed, every component's artifact is generated under:

```swift
artifacts/
    CsvExampleGen/examples/{run_id}/Split-*/ 
    StatisticsGen/statistics/{run_id}/Split-*/FeatureStats.pb
    SchemaGen/schema/{run_id}/schema.pbtxt
    ExampleValidator/anomalies/{run_id}/Split-*/SchemaDiff.pb
    Transform/...
```

This allowed us to dynamically load latest runs using helper functions like:

```python
def get_latest_subdir(directory):
    subdirs = [os.path.join(directory, o) for o in os.listdir(directory) if os.path.isdir(os.path.join(directory, o))]
    if not subdirs:
        raise FileNotFoundError(f"No subdirectories found in {directory}")
    return max(subdirs, key=os.path.getmtime)
```

### Visualization of Dataset Statistics

I used tensorflow_data_validation (tfdv) to load and visualize statistics:

```python
train_stats = tfdv.load_statistics(os.path.join(stats_path, "Split-train", "FeatureStats.pb"))
tfdv.visualize_statistics(train_stats)
```

Output:

- Rich interactive charts

- Feature distributions

- Missing value analysis

- Data type distributions

### Schema Visualization

We also displayed the schema:

```python
schema = tfdv.load_schema_text(os.path.join(schema_path, "schema.pbtxt"))
tfdv.display_schema(schema)
```

Schema gave us:

- Inferred types

- Domains (min/max)

- Presence constraints

### Anomalies Detection

ExampleValidator anomalies were visualized as:

```python
anomalies = tfdv.load_anomalies_text(os.path.join(anomalies_path, "SchemaDiff.pb"))
tfdv.display_anomalies(anomalies)
```

Result:

- Quickly spotted missing values, type mismatches, or distribution drifts

---

