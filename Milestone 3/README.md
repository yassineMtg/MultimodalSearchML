
# Multimodal Search ML

---

# Milestone 3

## Overview

This milestone focuses on building the data pipeline for the MultimodalSearchML project. We automated the ingestion, validation, preprocessing, and preparation of the dataset using TFX, ensured proper data versioning with DVC, and managed features with Feast as our feature store.

---

## Data Ingestion

- **Library used**: TFX ExampleGen

- **Dataset used:** query_features_with_timestamp.csv

- **Task**: Ingest raw data into the pipeline for future steps (statistics, schema generation, validation, and transformation).

I created a TFX pipeline component for ingestion using CsvExampleGen. Then, I loaded the dataset from:

```bash
data/query/query_features_with_timestamp.csv
```

The dataset is already prepared with query_id, f0 to f767 features, and an event_timestamp column.

```python
from tfx.components import CsvExampleGen
from tfx.proto import example_gen_pb2

input_config = example_gen_pb2.Input(splits=[
    example_gen_pb2.Input.Split(name='train', pattern='query_features_with_timestamp.csv'),
])

example_gen = CsvExampleGen(input_base=data_path, input_config=input_config)
```

- The CsvExampleGen is used to automatically:

	- Ingest the CSV file

	- Convert it into TFRecords

	- Automatically split into train and eval sets

	- Store generated artifacts under artifacts/CsvExampleGen

- Results:

You can observe the ingested data under:

```bash
tfx_pipeline/artifacts/CsvExampleGen/examples/
```

and inside:

```mathematica
Split-train/
Split-eval/
```

Each containing generated data_tfrecord-* files.

---

## Data Validation 

To automatically generate statistics, infer a data schema, and detect anomalies in the ingested query dataset before applying transformations or training models.

I added three essential components:

1. StatisticsGen: Computes descriptive statistics on the ingested data.

2. SchemaGen: Automatically infers the schema from the statistics.

3. ExampleValidator: Detects data anomalies and schema drift.

```python
from tfx.components import StatisticsGen, SchemaGen, ExampleValidator

# Compute statistics
statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

# Infer schema
schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])

# Validate dataset against the inferred schema
example_validator = ExampleValidator(
    statistics=statistics_gen.outputs['statistics'],
    schema=schema_gen.outputs['schema']
)
```

Pipeline Flow:

- Statistics are automatically computed from the TFRecords generated by ExampleGen.

- SchemaGen infers a schema without any manual intervention.

- ExampleValidator checks if:

    - The data conforms to the inferred schema.

    - There are missing values, unusual distributions, or unexpected types.

Results:

- Statistics are stored under:

```bash
tfx_pipeline/artifacts/StatisticsGen/statistics/
```

- Schema is stored under:

```bash
tfx_pipeline/artifacts/SchemaGen/schema/
```

This is the output of our schema: (schame.pbtxt)

```mathematica
feature {
  name: "event_timestamp"
  type: BYTES
  domain: "event_timestamp"
  presence {
    min_fraction: 1.0
    min_count: 1
  }
  shape {
    dim {
      size: 1
    }
  }
}
feature {
  name: "f0"
  type: FLOAT
  presence {
    min_fraction: 1.0
    min_count: 1
  }
  shape {
    dim {
      size: 1
    }
  }
}
feature {
  name: "f1"
  type: FLOAT
  presence {
    min_fraction: 1.0
    min_count: 1
  }
  shape {
    dim {
      size: 1
    }
  }
}
.....
.....
feature {
  name: "f767"
  type: FLOAT
  presence {
    min_fraction: 1.0
    min_count: 1
  }
  shape {
    dim {
      size: 1
    }
  }
}
feature {
  name: "query_id"
  type: INT
  presence {
    min_fraction: 1.0
    min_count: 1
  }
  shape {
    dim {
      size: 1
    }
  }
}
string_domain {
  name: "event_timestamp"
  value: "2025-03-27 07:01:04.332726+00:00"
}
```

Anomalies report is stored under:

```bash
tfx_pipeline/artifacts/ExampleValidator/anomalies/
```

NB:

- This process is fully automated thanks to TensorFlow Data Validation (TFDV).
- The schema file (schema.pbtxt) will serve as input for the next Transform step.
- Any anomalies detected can be analyzed in TensorBoard or directly via the generated artifacts.



